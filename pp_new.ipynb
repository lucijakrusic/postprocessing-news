{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGYhNzgYwgA8"
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715540964981,
     "user": {
      "displayName": "lucija kruÅ¡iÄ‡",
      "userId": "11566574528568305908"
     },
     "user_tz": -120
    },
    "id": "QlOQS2-J8plC",
    "outputId": "582bb0bb-5fae-4596-a27d-bb322e59438b"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('my_logger')\n",
    "\n",
    "logging.basicConfig()\n",
    "\n",
    "logger.debug('This is my ðŸ˜‚ debug message ')\n",
    "logger.info('This is my ðŸ’œ info message ')\n",
    "logger.warning('This is my ðŸ¤” warning message ')\n",
    "logger.error('This is my error ðŸ˜±message ')\n",
    "logger.critical('This is my ðŸ˜­ critical message ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISbxQUbx33by"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1715540967556,
     "user": {
      "displayName": "lucija kruÅ¡iÄ‡",
      "userId": "11566574528568305908"
     },
     "user_tz": -120
    },
    "id": "MeMezbV7ngGD",
    "outputId": "95f7a452-89b9-4371-ee8e-02199dd2e274"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNU--r8EC9Bz"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, set_start_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDep1lJ4sitY"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8lNAKXL5z3D"
   },
   "source": [
    "## In case you only want to try postprocessing with a small piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaDDP4BAs5FY"
   },
   "outputs": [],
   "source": [
    "text = '''Wir haben durch deÂ» Mund des Vorredners (Dumreicher) eben\n",
    "gehÃ¶rt, was fÃ¼r Elemente sich in den Ministerien befinden. Ich weiÃŸ\n",
    "nicht, wie Viele heute noch im UuterrichtsiiÃ¼nisterium sich vorsindcÂ»,\n",
    "die, wenn sie heute ihre Posten verlassen wÃ¼rden oder mÃ¼ÃŸten, alle\n",
    "wahrscheinlich ihren Platz dort (links) aufsuchen wÃ¼rden und die uns,\n",
    "von HaÃŸ erfÃ¼llt und erregt, Beschwerden zuschicken als Beweis ihrer\n",
    "gerechten Gesinnung fÃ¼r das slavischÂ« Element. (So ist es! reckts.)\n",
    "Wenn wir eine solche Bureaukratie haben, dann kÃ¶nnen wir aller\n",
    "dings nicht erwarten, daÃŸ unserÂ« Anliegen in gerechter, unparteiischer\n",
    "Weise erledigt werden (So ist es! rechts), und wenn dann gar so\n",
    "ein Cherusker dazukommt und in dem bÃ¶hmischen Schulwesen zu\n",
    "hausen beginnt mit seinem Furor tentooicus â€” dann Gnade den\n",
    "bÃ¶hmischen Schulen! (Bravo! rechts.) Redner kommt nun auf die\n",
    "Autonomie der LÃ¤nder in Bezug auf das Volksschulwesen zu sprechen\n",
    "und bemerkt: Herr v. Schmerling war in dieser Beziehung viel\n",
    "autonomistischer und viel liberaler, als alle ktaatsinÃ¤nner, die nach\n",
    "ihm das Unterrichtsministerium leiteten, viel autonomistischer, ver\n",
    "fassungsmÃ¤ÃŸiger und correcter, als die ReichsrÃ¤the, die hier Legis\n",
    "lation wachten. Das Reichs-Vvlksschulgesetz ist in ganz incorrecter\n",
    "Weise entstanden und stellt eineÂ» Bruck der Verfassung dar (Bravo!\n",
    "Bravo! rechts), und wir und das Ministerium befinden uns bei\n",
    "Fefthaltung dieses Gesetzes in eineÂ» unausgesetzten, in einem\n",
    "perennirenden VerfaffungSbruche (Sehr gut! rechts), denn wenn die\n",
    "Verfassung eine Wahrheit ist, so kann dieses Schulgesetz nicht aufrecht\n",
    "besteheÂ«, es muÃŸ abgeÃ¤ndert werden, es muÃŸ deÂ» LÃ¤ndern und VÃ¶lkern\n",
    "die Autonomie in Bezug auf das Schulwesen wieder zurÃ¼ckgegeben\n",
    "werden, denen man Â«s rncÂ«rerterwrise entzogen hat. (Beifall rechts.)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgBFcUxNRMSP"
   },
   "source": [
    "- problems around the '<-pad->' - tends to remove some words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkL586YgRHNa"
   },
   "source": [
    "Frpm the original notebook - just to test out the model without padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlGSctCKRGP3"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19929,
     "status": "ok",
     "timestamp": 1715541082900,
     "user": {
      "displayName": "lucija kruÅ¡iÄ‡",
      "userId": "11566574528568305908"
     },
     "user_tz": -120
    },
    "id": "8Oqziz9XUM9X",
    "outputId": "dd710647-98cf-4464-ebf2-2993ad5d6e1c"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"lukru/hmbyt-NewsEyeAnno\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"lukru/hmbyt-NewsEyeAnno\")#.to(\"cuda\")\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1715541082900,
     "user": {
      "displayName": "lucija kruÅ¡iÄ‡",
      "userId": "11566574528568305908"
     },
     "user_tz": -120
    },
    "id": "eF0s-M6uURbK",
    "outputId": "184bc30c-5714-4165-ce62-247434265d7e"
   },
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2jgR_hqT04q"
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens, tokenizer):\n",
    "    logging.info(\"Starting text chunking\")\n",
    "    text = text.replace('\\n', ' ')\n",
    "    sentences = re.split(r'(?<=[.?!;])\\s', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_chunk_token_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        logging.debug(f\"Processing sentence: {sentence[:30]}...\")\n",
    "\n",
    "        if len(sentence_tokens) > max_tokens:\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if current_chunk_token_count + len(word_tokens) <= max_tokens:\n",
    "                    current_chunk += (word + \" \")\n",
    "                    current_chunk_token_count += len(word_tokens)\n",
    "                else:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = word + \" \"\n",
    "                    current_chunk_token_count = len(word_tokens)\n",
    "        else:\n",
    "            if current_chunk_token_count + len(sentence_tokens) <= max_tokens:\n",
    "                current_chunk += (sentence + \" \")\n",
    "                current_chunk_token_count += len(sentence_tokens)\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "                current_chunk_token_count = len(sentence_tokens)\n",
    "\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "error",
     "timestamp": 1715541144700,
     "user": {
      "displayName": "lucija kruÅ¡iÄ‡",
      "userId": "11566574528568305908"
     },
     "user_tz": -120
    },
    "id": "piHk--onVIb3",
    "outputId": "0fdab94b-2967-4cea-d3b1-8c01bf78917e"
   },
   "outputs": [],
   "source": [
    "chunks = chunk_text(text, 150, tokenizer)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ga6wUtdmT4LD"
   },
   "outputs": [],
   "source": [
    "def process_text_file(text, max_tokens=150, tokenizer=tokenizer,model=model):\n",
    "    logging.info(\"Starting processing of text file\")\n",
    "    chunks = chunk_text(text, max_tokens, tokenizer)\n",
    "    processed_text = ''\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        logging.debug(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.to('cuda')\n",
    "\n",
    "        outputs = model.generate(inputs, max_new_tokens=1000, do_sample=True, top_p=0.95)\n",
    "        processed_chunk = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        processed_text += processed_chunk.strip() + \" \"\n",
    "\n",
    "    processed_text = processed_text.rstrip()\n",
    "\n",
    "    logging.info(\"Text file processing complete\")\n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17708,
     "status": "ok",
     "timestamp": 1714339700100,
     "user": {
      "displayName": "lucija kruÅ¡iÄ‡",
      "userId": "11566574528568305908"
     },
     "user_tz": -120
    },
    "id": "FiBDLdlXUlRT",
    "outputId": "74a22b8e-8351-4f22-df10-e84698ebae90"
   },
   "outputs": [],
   "source": [
    "print(process_text_file(text,max_tokens=150, tokenizer=tokenizer,model=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iy-t8vv6-iS"
   },
   "source": [
    "Now I split all relevant newspapers into smallers chunks, where each chunk is one row of the 'text' column in a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7B9jBl2gQxIm"
   },
   "source": [
    "for the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mN8f69RQzh-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xVEBcxxRSRu"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('drive/MyDrive/nfp_vtl_analysis/csvs/nfp/czechs_topic3_nfp.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1715592656564,
     "user": {
      "displayName": "lucija kruÅ¡iÄ‡",
      "userId": "11566574528568305908"
     },
     "user_tz": -120
    },
    "id": "qaZ7etAiNk-t",
    "outputId": "1ac8d187-6f5e-4641-e3fb-ede7eae07223"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wYGJTOJQ1Ge"
   },
   "outputs": [],
   "source": [
    "def process_dataframe(df,output_csv, max_tokens=150,tokenizer=None, model=None):\n",
    "    print(\"Starting processing of DataFrame\")\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['Document']\n",
    "        processed_text = process_text_file(text, max_tokens, tokenizer, model)\n",
    "\n",
    "\n",
    "        df.at[index, 'processed_text'] = processed_text\n",
    "\n",
    "    print(\"Completed processing DataFrame\")\n",
    "\n",
    "    df.to_csv(output_csv, encoding='utf-8')\n",
    "    print(\"DataFrame saved to pickle file\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a63nRqCjS7gY"
   },
   "outputs": [],
   "source": [
    "output_csv = 'drive/MyDrive/nfp_vtl_analysis/csvs/nfp/czechs_topic3_nfp_pp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5344069,
     "status": "ok",
     "timestamp": 1715599713878,
     "user": {
      "displayName": "lucija kruÅ¡iÄ‡",
      "userId": "11566574528568305908"
     },
     "user_tz": -120
    },
    "id": "l20-fqknQ34G",
    "outputId": "1518eadc-f616-421a-f91b-7242ebe9aa94"
   },
   "outputs": [],
   "source": [
    "process_dataframe(df, output_csv, max_tokens=150, tokenizer=tokenizer, model=model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8ETScZGTuGw"
   },
   "source": [
    "## Postprocessing of the whole file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4oEaQJFWwhh"
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEn8ZiAfW0k6"
   },
   "outputs": [],
   "source": [
    "def write_file(content, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CJSahYfW5aR"
   },
   "outputs": [],
   "source": [
    "def process_file(input_file_path, output_file_path, max_tokens=150, tokenizer=tokenizer, model=model):\n",
    "    text = read_file(input_file_path)\n",
    "    processed_text = process_text_file(text, max_tokens, tokenizer)\n",
    "    write_file(processed_text, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1706608440470,
     "user": {
      "displayName": "lucija kruÅ¡iÄ‡",
      "userId": "11566574528568305908"
     },
     "user_tz": -60
    },
    "id": "y3DCpICZW6sx",
    "outputId": "53a70648-154a-4d87-afe9-8f528fc1743e"
   },
   "outputs": [],
   "source": [
    "input_file_path = '/content/vtl18600915_merged.txt'\n",
    "output_file_path = '/content/vtl18600915_postprocessed.txt'\n",
    "# replace the paths with your own\n",
    "\n",
    "process_file(input_file_path, output_file_path, max_tokens=150, tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olqnlYJsYdgI"
   },
   "source": [
    "## Postprocessing of all files in a folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSJFAL2mYe5Y"
   },
   "outputs": [],
   "source": [
    "def list_txt_files(folder_path):\n",
    "    return [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkcEC81Tw6v4"
   },
   "outputs": [],
   "source": [
    "source_folder = '/content/drive/MyDrive/test'  # Replace with your source folder path\n",
    "target_folder = '/content/drive/MyDrive/test_nfp_pp'  # Replace with your target folder path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcUPXWE3xT3b"
   },
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGpjFAlOYhj7"
   },
   "outputs": [],
   "source": [
    "def process_all_files(source_folder, target_folder, max_tokens=150, tokenizer=tokenizer, model=model):\n",
    "    setup_logging()\n",
    "    print(\"Starting processing of all files\")\n",
    "\n",
    "\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    files = list_txt_files(source_folder)\n",
    "    print(f\"Found {len(files)} files to process\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_file = {executor.submit(process_file, os.path.join(source_folder, file), os.path.join(target_folder, file.replace('_merged.txt', '_postprocessed.txt')), max_tokens, tokenizer, model): file for file in files}\n",
    "        print('file submitted')\n",
    "\n",
    "        for future in as_completed(future_to_file):\n",
    "            file = future_to_file[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f\"Successfully processed '{file}' and saved to '{target_folder}'\")\n",
    "            except Exception as exc:\n",
    "                print(f\"File {file} generated an exception: {exc}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Completed processing all files. Total time taken: {total_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "executionInfo": {
     "elapsed": 16172,
     "status": "error",
     "timestamp": 1706608484363,
     "user": {
      "displayName": "lucija kruÅ¡iÄ‡",
      "userId": "11566574528568305908"
     },
     "user_tz": -60
    },
    "id": "mbWs7HoRllaY",
    "outputId": "56e1b191-1eb7-43f1-ad27-65552ac5f0df"
   },
   "outputs": [],
   "source": [
    "process_all_files(source_folder, target_folder, max_tokens=150, tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-cCejYvYktA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Process all .txt files in the source folder\n",
    "process_all_files(source_folder, target_folder, max_tokens=150, tokenizer=tokenizer, model=model)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNHMzVVQpEKjd1Czb+Q67CI",
   "gpuType": "V100",
   "machine_shape": "hm",
   "mount_file_id": "1kxe-AmqJsodARB9o_-EQpURjhHKE6BxZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
